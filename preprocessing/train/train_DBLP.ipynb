{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "We should add root directory to path so we can import our model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../..\")))\n",
    "import importlib\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from model.models import *\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import structured_negative_sampling, dropout_node\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import doc2vec\n",
    "import random\n",
    "from itertools import combinations\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read config YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../dblp.yaml\"\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['saving_paths']['graph'], 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "with open(config['saving_paths']['all_skills'], 'rb') as f:\n",
    "    all_skills = pickle.load(f)\n",
    "with open(config['saving_paths']['authors_id'], 'rb') as f:\n",
    "    author2id = pickle.load(f)\n",
    "with open(config['dblp_pickle'], \"rb\") as f:\n",
    "    dblp_data = pickle.load(f)\n",
    "id2author = {v:k for k,v in author2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "for item in tqdm(dblp_data):\n",
    "    authors = []\n",
    "    for author in item['authors']:\n",
    "        author = author.strip()\n",
    "        if author not in author2id:\n",
    "            continue\n",
    "        author_id = author2id[author]\n",
    "        authors.append(author_id)\n",
    "    if len(authors) == 0:\n",
    "        continue\n",
    "    \n",
    "    title = item['title']\n",
    "    abstract = item['abstract']\n",
    "    papers.append({'paper': title + \" \" + abstract, \"authors\": authors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_papers = []\n",
    "preprocessed_authors = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i, paper in enumerate(tqdm(papers)):\n",
    "    text = paper['paper']\n",
    "    text = text.strip().split()\n",
    "    words = list(filter(lambda x: x in all_skills, [lemmatizer.lemmatize(word) for word in text]))\n",
    "    if len(words) >= 2:\n",
    "        preprocessed_papers.append(doc2vec.TaggedDocument(words, [len(preprocessed_papers)]))\n",
    "        preprocessed_authors.append(paper['authors'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = doc2vec.Doc2Vec(preprocessed_papers, vector_size=64)\n",
    "with open(config['saving_paths']['doc2vec'], 'wb') as f:\n",
    "    pickle.dump(doc2vec_model, f)\n",
    "with open(config['train']['preprocessed_authors_path'], 'wb') as f:\n",
    "    pickle.dump(preprocessed_authors, f)\n",
    "with open(config['train']['preprocessed_papers_path'], 'wb') as f:\n",
    "    pickle.dump(preprocessed_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['saving_paths']['doc2vec'], 'rb') as f:\n",
    "    doc2vec_model = pickle.load(f)\n",
    "with open(config['train']['preprocessed_authors_path'], 'rb') as f:\n",
    "    preprocessed_authors = pickle.load(f)\n",
    "with open(config['train']['preprocessed_paper_path'], 'rb') as f:\n",
    "    preprocessed_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we select papers with more than 1 author as \"good\" papers which we use for training as \"queries\".\n",
    "For each query, we train the model so the embeddings of its authors are closer to the query than other random individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_papers = []\n",
    "for paper_index, paper_authors in enumerate(preprocessed_authors):\n",
    "    if len(paper_authors) > 1:\n",
    "        good_papers.append(paper_index)\n",
    "len(good_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tensors from networkx graph of collaboration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vec = torch_geometric.utils.from_networkx(graph)\n",
    "graph_x = graph_vec.x.float().to(device)\n",
    "graph_edge_index = graph_vec.edge_index.to(device)\n",
    "\n",
    "# This dict shows the mapping from graph node ids to corresponding indices of the created tensors.\n",
    "mapping = dict(zip(graph.nodes(), range(graph.number_of_nodes())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization\n",
    "We initialize the model using the model name and parameters that are provided in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['train']['model_name']\n",
    "models_module = importlib.import_module(\"model.models\")\n",
    "model_class = getattr(models_module, model_name)\n",
    "model_parameters = config['train']['model_params']\n",
    "learning_rate = config['train']['learning_rate']\n",
    "\n",
    "model = model_class(*model_parameters)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/kgolazde/exes/preprocessing/train', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/kgolazde/exes/venv/lib/python3.10/site-packages', '/home/kgolazde/exes']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['train']['batch_size']\n",
    "num_epochs = config['train']['num_epochs']\n",
    "num_negative_samples = config['train']['num_negative_samples']\n",
    "criterion = torch.nn.CosineEmbeddingLoss(margin=0)\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(range(0, len(good_papers), batch_size)):\n",
    "        optim.zero_grad()\n",
    "        graph_edge_index, _, _ = dropout_node(graph_edge_index)\n",
    "        graph_edge_index = graph_edge_index.to(device)\n",
    "        emb = model(graph_x, graph_edge_index)\n",
    "\n",
    "        batch_papers = good_papers[batch:batch + batch_size]\n",
    "        batch_items = []    # Embeddings of the positive and negative samples\n",
    "        query_emb = []\n",
    "        batch_labels = []   # 1 for positive and -1 for negative samples\n",
    "        for paper in batch_papers:\n",
    "            q_emb = torch.Tensor(doc2vec_model.dv.vectors[paper]).to(device)\n",
    "            for author in preprocessed_authors[paper]:\n",
    "                if author not in mapping:\n",
    "                    continue\n",
    "                batch_items.append(emb[mapping[author]])\n",
    "                negs = []\n",
    "                while len(negs) < num_negative_samples:\n",
    "                    neg = sample(graph.nodes, 1)[0]\n",
    "                    while neg in preprocessed_authors[paper] or neg not in mapping or neg in negs:\n",
    "                        neg = sample(graph.nodes, 1)[0]\n",
    "                    negs.append(neg)\n",
    "                for neg in negs:\n",
    "                    batch_items.append(emb[mapping[neg]])\n",
    "                batch_labels.extend([1] + ([-1] * num_negative_samples))\n",
    "                for i in range(num_negative_samples + 1):\n",
    "                    query_emb.append(q_emb)\n",
    "        query_emb = torch.stack(query_emb)\n",
    "        batch_items = torch.stack(batch_items)\n",
    "        batch_labels = torch.Tensor(batch_labels).to(device).detach()  \n",
    "        loss = criterion(query_emb, batch_items, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saving_dir = config['train']['model_saving_dir']\n",
    "model_saving_path = f\"{model_saving_dir}/DBLP_{model_name}.pt\"\n",
    "torch.save(model.state_dict(), model_saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Link prediction model\n",
    "\n",
    "We use the GAE model from torch_geometric for Link Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GAE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "link_prediction_model_params = config['link_prediction']['model_params']\n",
    "gae_model = GAE(LinkPredictionModel(*link_prediction_model_params))\n",
    "gae_optim = torch.optim.Adam(gae_model.parameters(), lr=1e-2)\n",
    "\n",
    "train_data, _, test_data = RandomLinkSplit(num_val=0,split_labels=True)(graph_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_prediction_epochs = config['link_prediction']['num_epochs']\n",
    "for _ in range(link_prediction_epochs):\n",
    "    gae_model.train()\n",
    "    gae_model.to(device)\n",
    "    gae_optim.zero_grad()\n",
    "    z = gae_model.encode(train_data.x.float().cuda(), train_data.edge_index.cuda())\n",
    "\n",
    "    loss = gae_model.recon_loss(z, pos_edge_index=train_data.pos_edge_label_index, neg_edge_index=train_data.neg_edge_label_index)\n",
    "    print(\"loss:\", loss.item(), end=\" \")\n",
    "    loss.backward()\n",
    "    gae_optim.step()\n",
    "    gae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        gae_model.eval()\n",
    "        z = gae_model.encode(test_data.x.float().cuda(), test_data.edge_index.cuda())\n",
    "        print(gae_model.test(z, test_data.pos_edge_label_index, test_data.neg_edge_label_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Link Prediction model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_prediction_saving_dir = config['link_prediction']['model_saving_dir']\n",
    "link_prediction_saving_path = f\"{link_prediction_saving_dir}/DBLP_GAE.pt\"\n",
    "\n",
    "torch.save(gae_model.state_dict(), link_prediction_saving_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
