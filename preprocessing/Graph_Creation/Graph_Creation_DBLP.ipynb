{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7d94e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import torch\n",
    "import json\n",
    "import torch_geometric\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2f9ce",
   "metadata": {},
   "source": [
    "# Read config YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66eddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../dblp.yaml\"\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814d3ae",
   "metadata": {},
   "source": [
    "# Parse DBLP pickle file\n",
    "The DBLP file contains a list of papers with its authors and the abstract. First we gather all papers and all collaborators for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['dblp_pickle'], \"rb\") as f:\n",
    "    dblp_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_id = {}\n",
    "author_papers = {}\n",
    "author_colabs = {}\n",
    "\n",
    "# Gather papers of each author\n",
    "for paper in tqdm(dblp_data):\n",
    "    for author in paper['authors']:\n",
    "        author = author.strip()\n",
    "        if author not in author_id:\n",
    "            new_id = len(author_id)\n",
    "            author_id[author] = new_id\n",
    "        aid = author_id[author]\n",
    "        if aid not in author_papers:\n",
    "            author_papers[aid] = []\n",
    "        author_papers[aid].append({\"title\": paper['title'], \"abstract\": paper[\"abstract\"]})\n",
    "\n",
    "# Find collaborators of each author\n",
    "for item in tqdm(dblp_data):\n",
    "    for author in item['authors']:\n",
    "        author = author.strip()\n",
    "        aid = author_id[author]\n",
    "        if aid not in author_colabs:\n",
    "            author_colabs[aid] = []\n",
    "        for other in item['authors']:\n",
    "            other = other.strip()\n",
    "            oid = author_id[other]\n",
    "            if oid != aid:\n",
    "                author_colabs[aid].append(oid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d161dc2",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We filter the authors using a list of authors from previous work.\n",
    "\n",
    "We keep collaborations that have been repeated for at least 5 times. Also, we keep authors that have at least 10 papers and 3 collaborators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bd7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['good_authors'], \"rb\") as f:\n",
    "    good_authors = pickle.load(f)\n",
    "\n",
    "for author in author_id.values():\n",
    "    counter = dict(Counter(author_colabs[author]))\n",
    "    author_colabs[aid] = [item for item, cnt in counter.items() if cnt > 5]\n",
    "\n",
    "author_id_temp = {}\n",
    "for author in good_authors:\n",
    "    if len(author_colabs[author_id[author]]) >= 3:\n",
    "        author_id_temp[author] = author_id[author]\n",
    "author_id = author_id_temp\n",
    "\n",
    "author_colabs = {id: author_colabs[id] for id in author_id.values()}\n",
    "\n",
    "id_author = {v:k for k,v in author_id.items()}\n",
    "filtered_authors = [id for id in author_id.values() if 10 <= len(author_papers[id])]\n",
    "len(filtered_authors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bde7edcc",
   "metadata": {},
   "source": [
    "## Keyword Extraction\n",
    "\n",
    "This section finds the universal skill set $S$ using KeyBERT. If this set already exists in the dataset, this section could be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fc01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "keybert_model = KeyBERT(model='allenai-specter')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = []\n",
    "for author in filtered_authors:\n",
    "    papers = [item['title'] for item in author_papers[author]]\n",
    "    total_text.extend(papers)\n",
    "\n",
    "number_of_keywords_per_paper = 5\n",
    "paper_keywords = keybert_model.extract_keywords(total_text, top_n=number_of_keywords_per_paper)\n",
    "\n",
    "with open(config['paper_keywords_path'], \"wb\") as f:\n",
    "    pickle.dump(paper_keywords, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceefd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['paper_keywords_path'], \"rb\") as f:\n",
    "    paper_keywords = pickle.load(f)\n",
    "\n",
    "counter = Counter()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for ind, keywords in enumerate(tqdm(paper_keywords)):\n",
    "    for kw in keywords:\n",
    "        lemm = lemmatizer.lemmatize(kw[0])\n",
    "        counter.update([lemm])\n",
    "\n",
    "total_skills_count = 2000\n",
    "all_skills = [kw[0] for kw in counter.most_common(total_skills_count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7567a1a",
   "metadata": {},
   "source": [
    "# Find skills of each expert\n",
    "\n",
    "In this section, we concatenate all text (paper titles and abstracts) by each author, and then find TF-IDF values for each author's corresponding text. Then, we assign skills to experts when their TF-IDF values surpass a predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for id in tqdm(filtered_authors):\n",
    "    author_text = [paper['title'] + \" \" + paper['abstract'] for paper in author_papers[id]]\n",
    "    author_text = ' '.join(author_text)\n",
    "    corpus.append(author_text) \n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=all_skills)\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "tfidf_vecs = vectorizer.transform(corpus).toarray()\n",
    "tfidf_threshold = config['tfidf_threshold']\n",
    "\n",
    "expert_skills = {}\n",
    "for ind, item_skills in enumerate(tqdm(tfidf_vecs)):\n",
    "    id = filtered_authors[ind]\n",
    "    skills_boolean = (item_skills > tfidf_threshold)    \n",
    "    skills_list = [int(v) for v in skills_boolean]\n",
    "    expert_skills[id] = skills_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a2f09",
   "metadata": {},
   "source": [
    "Then, we eliminate skills that are too rare or common from the universal skill set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_author = {} # This dict contains skills and authors who possess them.\n",
    "for ind, skill in enumerate(tqdm(all_skills)):\n",
    "    skill_author[skill] = []\n",
    "    \n",
    "    for author in expert_skills:\n",
    "        if expert_skills[author][ind] > 0:\n",
    "            skill_author[skill].append(author)\n",
    "\n",
    "skill_count = [(k, len(v)) for k,v in skill_author.items()]\n",
    "\n",
    "# We only keep skills that are possessed by between 20 and 2200 experts.\n",
    "good_skills = [(skill, all_skills.index(skill)) for (skill, count) in skill_count if 20 < count < 2200]\n",
    "good_skills_indices = [a[1] for a in good_skills]\n",
    "\n",
    "# We remove eliminated skills from experts' vectors\n",
    "all_skills = [all_skills[i] for i in good_skills_indices]\n",
    "for author in tqdm(expert_skills):\n",
    "    skills = expert_skills[author]\n",
    "    skills = [skills[ind] for ind in good_skills_indices]\n",
    "    expert_skills[author] = skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc1a89",
   "metadata": {},
   "source": [
    "# Create Collaboration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of edges\n",
    "edges = []\n",
    "\n",
    "for i in tqdm(expert_skills):\n",
    "    for j in author_colabs[i]:\n",
    "        if j in expert_skills:\n",
    "            edges.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ded61",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "g.add_edges_from(edges)\n",
    "nx.set_node_attributes(g, expert_skills, name=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc7ada",
   "metadata": {},
   "source": [
    "# Save collaboration network to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['saving_paths']['graph'], \"wb\") as f:\n",
    "    pickle.dump(g, f)\n",
    "with open(config['saving_paths']['authors_id'], \"wb\") as f:\n",
    "    pickle.dump(author_id, f)\n",
    "with open(config['saving_paths']['all_skills'], \"wb\") as f:\n",
    "    pickle.dump(all_skills, f)\n",
    "with open(config['saving_paths']['vectorizer'], \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
