{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from github import Github\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read config YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../github.yaml\"\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Github API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_token = \"token\" # Put your GitHub API token here\n",
    "git_api = Github(github_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve list of repositories\n",
    "\n",
    "In this section, we gather the list of repositories that we want to crawl from two CSV datasets. We first clean the data from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = config['dataset_paths']\n",
    "repos = pd.read_csv(csv_paths[0])\n",
    "repos2 = pd.read_csv(csv_paths[1])\n",
    "repos = repos[repos.topic != \"awesome\"] # Filter out repositories from awesome pages.\n",
    "repos = repos[[\"topic\", \"name\", \"star\", \"topic_tag\", \"discription_text\", \"url\"]]\n",
    "repos2 = repos2[[\"Name\", \"URL\", \"Stars\", \"Topics\", \"Description\"]]\n",
    "repos = repos.rename(columns={\"discription_text\": \"description\", \"topic_tag\": \"tags\"})\n",
    "repos2 = repos2.rename(columns={\"Name\": \"name\", \"URL\": \"url\", \"Stars\": \"star\", \"Topics\": \"tags\", \"Description\": \"description\"})\n",
    "repos = pd.concat([repos, repos2]).drop_duplicates(subset=\"name\")\n",
    "repos = repos[~repos.name.str.contains(\"awesome|tutorial|interview|book|roadmap|list|cheat|how|best|book|scratch\", case=False, na=False)]\n",
    "\n",
    "# Convert strings which contain project tags to lists\n",
    "def convert_tags(item):\n",
    "    result = []\n",
    "    striped = item.strip(\"['\").strip(\"']\")\n",
    "    striped = striped.split(', ')\n",
    "    for tag in striped:\n",
    "        tag = tag.strip(\"'\")\n",
    "        result.append(tag)\n",
    "    return result\n",
    "repos.tags = repos.tags.apply(convert_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since repo stars are in format of 12.3k, we convert these strings to numbers.\n",
    "def value_to_float(value):\n",
    "    if type(value) == float or type(value) == int:\n",
    "        return value\n",
    "    if 'k' in value:\n",
    "        if len(value) > 1:\n",
    "            return float(value.replace('k', '')) * 1000\n",
    "        return 1000.0\n",
    "    return 0.0\n",
    "\n",
    "# We keep repositories with more than 2000 stars\n",
    "good_repos = repos[repos.star.apply(value_to_float) > 2000]\n",
    "good_repos = good_repos[good_repos.tags.apply(len) > 1]\n",
    "\n",
    "# Set project url as \"repoOwner/repoName\"\n",
    "urls = good_repos.url.apply(lambda item: \"/\".join(item.split(\"/\")[-2:]))\n",
    "good_repos['title'] = urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl repository contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dict = {}\n",
    "repo_dict_path = config['repo_dict_path']\n",
    "for url in tqdm(urls):\n",
    "    if url in repo_dict:\n",
    "        continue\n",
    "    try:\n",
    "        repo = git_api.get_repo(url)\n",
    "        contributor_users = list(repo.get_contributors())\n",
    "        repo_dict[url] = {\"repo\": repo, \"contributors\": contributor_users}\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "with open(repo_dict_path, \"wb\") as f:\n",
    "    pickle.dump(repo_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep users that had worked on more than 3 projects. Plus, we remove bots from our users list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(repo_dict_path, \"rb\") as f:\n",
    "    repo_dict = pickle.load(f)\n",
    "\n",
    "user_project_counter = Counter()\n",
    "for repo_url, repo_items in repo_dict.items():\n",
    "    contributor_users = repo_items['contributors']\n",
    "    contributor_users = [item.login for item in contributor_users]  # Get usernames of contributors\n",
    "    user_project_counter.update(contributor_users)\n",
    "good_users = [user for user in user_project_counter if user_project_counter[user] > 3 and '[bot]' not in user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "We extract keywords from project tags. First, we create the universal keyword list using keywords which are present in more than 1000 projects and are not too rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counter = Counter()\n",
    "for item in repos.tags:\n",
    "    for tag in item:\n",
    "        tag_counter.update([tag])\n",
    "good_tags = [item for item, cnt in tag_counter.items() if cnt > 100 and item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add programming languages used in each repo to its keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = set()\n",
    "repo_language = {}\n",
    "for repo_url, repo_items in repo_dict.items():\n",
    "    language = repo_items['repo'].language\n",
    "    if not language:\n",
    "        continue\n",
    "    language = language.lower().replace(\" \", \"-\")\n",
    "    languages.add(language)\n",
    "    url = \"/\".join(repo_items['repo'].url.split(\"/\")[-2:])\n",
    "    repo_language[url] = repo_items\n",
    "\n",
    "good_tags = list(set(good_tags + list(languages)))  # Append programming languages to universal keyword list\n",
    "good_repos['language'] = good_repos.title.apply(lambda x: [repo_language[x]] if x in repo_language else [])\n",
    "good_repos['tags'] = good_repos.apply(lambda x: x.tags + x.language, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find skills of each expert\n",
    "\n",
    "In this section, we concatenate all keywords in each user's repositories, and then find TF-IDF values for each users's corresponding keywords. Then, we assign skills to experts when their TF-IDF values surpass a predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_repos = defaultdict(list)\n",
    "for repo_url, repo_items in tqdm(repo_dict.items()):\n",
    "    contributor_users = repo_items['contributors']\n",
    "    for user in contributor_users:\n",
    "        user_repos[user.login].append(repo_url)\n",
    "user_repos = {k:v for k,v in user_repos.items() if k in good_users}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_skills = {}\n",
    "\n",
    "for user, repos in tqdm(user_repos.items()):\n",
    "    skills = []\n",
    "    for repo in repos:\n",
    "        rep = good_repos[good_repos.title == repo]\n",
    "        sk = filter(lambda i: i in good_tags, rep.tags.iloc[0])\n",
    "        skills.extend(sk)\n",
    "    user_skills[user] = skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=good_tags)\n",
    "tfidf_threshold = config['tfidf_threshold']\n",
    "\n",
    "all_docs = []\n",
    "for user, skills in user_skills.items():\n",
    "    skills_str = \" \".join(skills)\n",
    "    all_docs.append(skills_str)\n",
    "vectorizer.fit(all_docs)\n",
    "\n",
    "user_id_dict = {}\n",
    "users_skills_dict = {}\n",
    "for user, skills in tqdm(user_skills.items()):\n",
    "    if user not in user_id_dict:\n",
    "        user_id_dict[user] = len(user_id_dict)\n",
    "    user_id = user_id_dict[user]\n",
    "    skills_str = \" \".join(skills)\n",
    "    user_vec = vectorizer.transform([skills_str])\n",
    "    user_vec = (user_vec > tfidf_threshold).todense().tolist()[0]\n",
    "    user_vec = [1 if item else 0 for item in user_vec]\n",
    "    users_skills_dict[user_id] = user_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find collaborators of each user\n",
    "\n",
    "Here, we find collaborators of each individual in our dataset. We keep collaborations that have been repeated for more than 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_collab_counter = defaultdict(Counter)\n",
    "user_collaborators = {}\n",
    "\n",
    "for repo_url, repo_items in tqdm(repo_dict.items()):\n",
    "    for user in repo_items['contributors']:\n",
    "        if user.login not in user_id_dict:\n",
    "            continue\n",
    "        for other in repo_items['contributors']:\n",
    "            if other.login not in user_id_dict:\n",
    "                continue\n",
    "            if user != other:\n",
    "                user_collab_counter[user_id_dict[user.login]].update([user_id_dict[other.login]])\n",
    "\n",
    "for user, counter in user_collab_counter.items():\n",
    "    collabs = [other_user for other_user, count in counter.items() if count > 2]\n",
    "    user_collaborators[user] = collabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Collaboration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "edges = []\n",
    "for user, collabs in user_collaborators.items():\n",
    "    for other in collabs:\n",
    "        edges.append((user, other))\n",
    "g.add_edges_from(edges)\n",
    "nx.set_node_attributes(g, user_collaborators, name=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save collaboration network to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['saving_paths']['graph'], \"wb\") as f:\n",
    "    pickle.dump(g, f)\n",
    "with open(config['saving_paths']['authors_id'], \"wb\") as f:\n",
    "    pickle.dump(user_id_dict, f)\n",
    "with open(config['saving_paths']['all_skills'], \"wb\") as f:\n",
    "    pickle.dump(good_tags, f)\n",
    "with open(config['saving_paths']['vectorizer'], \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i, repo in tqdm(good_repos.iterrows()):\n",
    "    words = list(filter(lambda x: x in good_tags, repo.tags))\n",
    "    docs.append(doc2vec.TaggedDocument(words, [len(docs)]))\n",
    "\n",
    "doc2vec_model = doc2vec.Doc2Vec(docs, vector_size=64, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/doc2vec_github.pkl\", \"wb\") as f:\n",
    "    pickle.dump(doc2vec_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/user_repos.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_repos, f)\n",
    "with open(\"../data/good_repos.pkl\", \"wb\") as f:\n",
    "    pickle.dump(good_repos, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
